MODEL:
    n_head: 8
    n_layer: 12
    dropout: 0.1
    d_inner: 2048           # d_ff
    d_embed: 512
    d_model: 512
    dropatt: 0.0            # attention probability dropout rate
    query_dim: 16           # 64
    seq_len: 512            # 512
    mem_len: 512            # Corpus-related parameter
    ext_len: 0
    tgt_len: 70
    eval_tgt_len: 50
    init: 'normal'          # parameter initializer to use.
    emb_init: 'normal'      # parameter initializer to use.
    init_range: 0.1
    emb_init_range: 0.01    # parameters initialized by U(-init_range, init_range)
    init_std: 0.02          # parameters initialized by N(0, init_std)
    proj_init_std: 0.01
    clamp_len: -1           # use the same pos embeddings after clamp_len
    div_val: 1
    position_concat: False
    pre_lnorm: True         # apply LayerNorm to the input instead of the output
    same_length: True       # use the same attn length for all tokens


TRAIN: 
    gpuID: '1'
    train_id: 'wikifonia-melody'
    batch_size: 22
    lr: 0.001
    optim_adam_beta1: 0.9
    optim_adam_beta2: 0.98
    weight_decay: 0
    num_epochs: 200
    save_freq: 10
    seed: 2222
    optim: 'adam'
    no_cuda: False
    resume_training_model: False


INFERENCE:
    num_sample: 50
    gpuID: '2'
    train_id: 'wikifonia-melody'
    no_cuda: False
